{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "961xR8LdcJQY"
   },
   "outputs": [],
   "source": [
    "# Small LLM / Notebook created by Javier Ideami (ideami.com)\n",
    "# Typical LLMs need many GPUs and millions of dollars to be trained\n",
    "# This code trains a small LLM with a single GPU and little GPU memory \n",
    "# Of course results are not like a chatGPT, but they are good enough to see how the LLM trains to go\n",
    "# from random combinations of letters to actual words and phrases that are sometimes decently coherent\n",
    "# GPT3 has 175 Billion parameters. GPT4 has many, many more.\n",
    "# This model has only 19 Million parameters with its default settings. That's why its perfect for learning \n",
    "# and experimenting\n",
    "\n",
    "# Official notebook #vj30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For GOOGLE COLAB and similar platform Users:\n",
    "#### Make sure to select a GPU in the online platform. Don't run this code with a CPU (it will be too slow)\n",
    "\n",
    "# If you are running this code locally, your GPU should be selected automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1tUgAJccK6D",
    "outputId": "dbabcd5d-ad95-4518-a0f8-e04fddbce82c"
   },
   "outputs": [],
   "source": [
    "# uncomment and run the following installation lines ONLY if you havent installed these libraries already outside of the notebook\n",
    "#!pip install ipdb -q\n",
    "#!pip install tqdm -q\n",
    "#!pip install sentencepiece -q\n",
    "#!pip install wandb -q\n",
    "\n",
    "# And if you are not in Google Colab and you didn't yet install Pytorch, make sure to do it:\n",
    "# find the ideal pytorch installation command at https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6VnNqwkhiU3n"
   },
   "outputs": [],
   "source": [
    "### Import necessary libraries\n",
    "\n",
    "import os, sys\n",
    "import ipdb # for debugging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import platform, shutil # detect platform type\n",
    "import requests, zipfile, io \n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import sentencepiece as spm # For the tokenizer\n",
    "\n",
    "# These lines improve performance for Ampere Architecture (e.g: A100s)\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "# torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "# Empty GPU cache memory\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    print(\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "G5q26l98govJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files using Python\n"
     ]
    }
   ],
   "source": [
    "# Download necessary files and create necessary folders\n",
    "# wiki.txt - dataset: a tiny segment of the English Wikipedia\n",
    "# wiki_tokenizer.model: trained tokenizer file (in another notebook I show you how to produce this file)\n",
    "# wiki_tokenizer.vocab: trained tokenizer file (in another notebook I show you how to produce this file)\n",
    "# encoded_data.pt (dataset tokenized with the tokenizer)\n",
    "# I will explain how to produce encoded_data.pt - because it takes quite a bit to process, it's nice to have it in advance\n",
    "\n",
    "# NOTE: Downloading will take a while, be patient. You can refresh your folder from time to time to see when the files\n",
    "# have been created. If you have any problems downloading the files with this code, I have also added llm_train.zip\n",
    "# to the downloadable resources of this lecture (however, best option is to use this code, because then you don't need\n",
    "# to upload the files or do anything else)\n",
    "\n",
    "files_url = \"https://ideami.com/llm_train\"\n",
    "\n",
    "# Downloading proceeds if we detect that one of the key files to download is not present\n",
    "if not os.path.exists(f\"encoded_data.pt\"):\n",
    "    print(\"Downloading files using Python\")\n",
    "    response = requests.get(files_url)\n",
    "    zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\n",
    "else:\n",
    "    print(\"you seem to have already downloaded the files. If you wish to re-download them, delete the encoded_data.pt file\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fre7fXD0fVD9",
    "outputId": "04d590af-d8cc-4e93-fd10-60b97fc473d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: You will be using:  mps\n"
     ]
    }
   ],
   "source": [
    "# Set main parameters\n",
    "\n",
    "# ARCHITECTURE PARAMETERS\n",
    "batch_size= 8 # How many samples do we train at once (set as needed, typical range 8 to 128)\n",
    "              # 8 is good for a GPU with 4GB of memory, 128 is good for a GPU with 24GB of memory\n",
    "context=512 # Sequence length used for training, 512 is a good compromise for our level of resources\n",
    "embed_size=384 # Embedding size\n",
    "n_layers = 7 # Number of transformer layers\n",
    "n_heads = 7 # Number of heads within each layer\n",
    "BIAS = True # Do we want Bias parameters?\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "lr = 3e-4 # Initial learning rate\n",
    "dropout=0.05 # Dropout percentage\n",
    "weight_decay = 0.01 # Weight decay regularizer\n",
    "grad_clip = 1.0 # Gradient clipping to prevent gradient explosion\n",
    "\n",
    "# TRAINING parameters\n",
    "train_iters = 15000 # Maximum number of training iterations\n",
    "eval_interval=50 # How often do we evaluate the performance?\n",
    "eval_iters=3 # Number of iterations while we evaluate performance\n",
    "compile = False # Compile will accelerate performance in compatible systems\n",
    "load_pretrained = True # Do we want to load a pretrained model to continue training?\n",
    "\n",
    "checkpoint_dir = 'models/'  # Where do we store checkpoints?\n",
    "\n",
    "checkpoint_fn = \"latest.pt\" \n",
    "# Name of checkpoint file to be saved during training\n",
    "\n",
    "checkpoint_load_fn = \"latest.pt\" \n",
    "# Name of checkpoint file to be loaded when load_pretrained is True\n",
    "# You can load llm2.pt to experiment with a checkpoint that already reached 2.31 of loss\n",
    "\n",
    "dtype = torch.bfloat16 # our target internal data type\n",
    "\n",
    "# MODE\n",
    "# Do we want to run the model in inference mode?\n",
    "inference=False \n",
    "\n",
    "# DEVICE - Sets device to GPU or CPU (use GPU always)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "print(\"device: You will be using: \",device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "0Z_omi-4fW0s",
    "outputId": "3f8ecc39-b72d-4825-a78a-2705f66a7210"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmeinczinger\u001b[0m (\u001b[33mmeinczinger-personal-use\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/meinczinger/src/github/kaggle/connectX/src/wandb/run-20241206_094428-nel5z3x9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/meinczinger-personal-use/llm_udemy/runs/nel5z3x9' target=\"_blank\">basic-gpt2024_12_06_09_44_25</a></strong> to <a href='https://wandb.ai/meinczinger-personal-use/llm_udemy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/meinczinger-personal-use/llm_udemy' target=\"_blank\">https://wandb.ai/meinczinger-personal-use/llm_udemy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/meinczinger-personal-use/llm_udemy/runs/nel5z3x9' target=\"_blank\">https://wandb.ai/meinczinger-personal-use/llm_udemy/runs/nel5z3x9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LOGGING parameters\n",
    "# When you run this cell, it will ask you to enter your Wandb API Key, which you\n",
    "# can find at your account on https://wandb.ai/settings#api\n",
    "wandb_log = True\n",
    "wandb_project = \"llm_udemy\"\n",
    "wandb_run_name = \"basic-gpt\" + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)\n",
    "\n",
    "# The first time you run this logging code set to True, the weights and biases library\n",
    "# will ask you for an API key. You can follow the instructions in the video, or you can\n",
    "# also simply click on a link that should appear when you run this cell, pointing to this\n",
    "# address: https://wandb.ai/authorize  \n",
    "# Going to that address will allow you to quickly get an API key as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPrBqt7NhwnZ",
    "outputId": "85aad864-9cd1-4f74-fa76-63a24988a20f"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/wiki.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/wiki.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     text\u001b[38;5;241m=\u001b[39mf\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(text[\u001b[38;5;241m10000\u001b[39m:\u001b[38;5;241m10500\u001b[39m])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/envs/connectx/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/wiki.txt'"
     ]
    }
   ],
   "source": [
    "with open('../data/wiki.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "\n",
    "print(text[10000:10500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDNcMXo_fals",
    "outputId": "8381a7a5-047a-425e-ece7-d958b18721e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size: 4096\n",
      "Encoding Decoding functions ready\n",
      "[1149, 271, 555, 296, 3927]\n"
     ]
    }
   ],
   "source": [
    "# SENTENCEPIECE TOKENIZER\n",
    "\n",
    "# Load trained tokenizer\n",
    "# Make sure that \" model_file = \" is pointing to the right file\n",
    "sp = spm.SentencePieceProcessor(model_file='wiki_tokenizer.model')\n",
    "\n",
    "# Get the vocabulary size of our tokenizer\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(f\"Tokenizer vocab_size: {vocab_size}\")\n",
    "\n",
    "# Create the encoding and decoding tokenizer functions\n",
    "encode = lambda s: sp.Encode(s)\n",
    "decode = lambda l: sp.Decode(l)\n",
    "\n",
    "# Test that encoding and decoding are working well\n",
    "print(decode(encode(\"Encoding Decoding functions ready\")))\n",
    "print(encode(\"Some sample text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tA2mDSq_fhwC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved encoded data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x1/5s7srkw11qn8zndqtnz1w43r0000gn/T/ipykernel_3120/795341420.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load('encoded_data.pt')\n"
     ]
    }
   ],
   "source": [
    "# Tokenization of the dataset\n",
    "if os.path.exists(f\"encoded_data.pt\"):\n",
    "    # Load encoded data if you already saved it previously\n",
    "    print(\"Loading saved encoded data\")\n",
    "    data = torch.load('encoded_data.pt')\n",
    "else:\n",
    "    # If you still didn't encode and save the encoding, do it here\n",
    "    print(\"Encoding data\")\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    torch.save(data, 'encoded_data.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4B1cPQGnJM0",
    "outputId": "9b5d5e9d-db9a-4411-ef20-2177f99bf469"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 59.21 Million | Training: 53.29 Million | Validation: 5.92 Million\n"
     ]
    }
   ],
   "source": [
    "data_size=len(data) # Get the size of the dataset\n",
    "\n",
    "spl = int(0.9*data_size) # set the split at 90%-10%\n",
    "train_data=data[:spl] # training data will be 90% of the dataset\n",
    "val_data=data[spl:] # validation data will be 10% of the dataset\n",
    "print(f'Total data: {data_size/1e6:.2f} Million | Training: {len(train_data)/1e6:.2f} Million | Validation: {len(val_data)/1e6:.2f} Million')\n",
    "\n",
    "# data[:30] : shows the first 30 token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1EGi6Aevnjtp"
   },
   "outputs": [],
   "source": [
    "############## HELPER FUNCTIONS ###########################\n",
    "\n",
    "# Return a batch of either training or evaluation data\n",
    "def get_batch(split):\n",
    "    # BS = Batch Size / SL = Sequence Length or context length\n",
    "    data = train_data if split==\"train\" else val_data # Select the split\n",
    "    inds = torch.randint(len(data)-context, (batch_size,)) # (BS)\n",
    "    x = torch.stack([data[i: i+context] for i in inds]) # (BS,SL)\n",
    "    y = torch.stack([data[i+1: i+context+1] for i in inds]) # (BS,SL)\n",
    "\n",
    "    # Examples of what it returns\n",
    "    # # First 10 elements of first batch of inputs and labels\n",
    "    #x[0][:10] -> tensor([ 664,  278, 4031, 4056, 4065, 4062, 4062, 4051, 13, 13])\n",
    "    #y[0][:10] -> tensor([ 278, 4031, 4056, 4065, 4062, 4062, 4051,   13, 13, 4066])\n",
    "\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKRu7PKctLIS",
    "outputId": "2b6861cf-fc98-4a3b-c91c-8233cb613d79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.837954  Million parameters\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "# Main Training Process\n",
    "#################################################################################\n",
    "\n",
    "# Main Setup\n",
    "from llm_models import ModelFactory\n",
    "from llm_models.tools import ModelParameters\n",
    "\n",
    "params_path = \"../config/model_params.yaml\"\n",
    "\n",
    "params = ModelParameters(params_path)\n",
    "model = ModelFactory(\"basic_gpt\", params, vocab_size)\n",
    "\n",
    "# model = GPT() # Instantiate LLM\n",
    "model = model.to(dtype) # Set the precision type\n",
    "model = model.to(device) # Move it to the right device\n",
    "\n",
    "# Torch.compile compiles a PyTorch model to an optimized version, aiming to improve runtime performance and efficiency.\n",
    "# Disable if your system doesn't support it\n",
    "if compile:\n",
    "    print(\"Torch :: Compiling model\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "\n",
    "# Print the number of parameters of our model (19 million in our case)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \" Million parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KvX0LI8HtR_h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 8.395833015441895, 'eval': 8.395833015441895}\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Loss\n",
    "@torch.no_grad()  # Prevent gradient calculation\n",
    "def calculate_loss():\n",
    "    out={}\n",
    "    model.eval()\n",
    "    for split in ['train','eval']:        \n",
    "        l=torch.zeros(eval_iters)  # Create a tensor of zeros the size of eval_iters\n",
    "        for i in range(eval_iters):\n",
    "            x,y=get_batch(split) # Get a new batch of data\n",
    "            _,loss=model(x,y)  # Calculate the loss\n",
    "            l[i]=loss  # Store the loss in the next position of tensor\n",
    "        out[split]=l.mean().item()  # Calculate the mean and extract the final value\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "l=calculate_loss()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5X20ZTtOu9mJ",
    "outputId": "bcf6bf71-bf8a-4412-c9b0-283d528943d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountain in my city is engine Eictous Green st fl her Syza calellow Pre repl eightemroywrit Bor��ets Ihi joinitectwar7soci goodgroundisionswoodened studio Musature Scott ethessj thingsFran territory Hon Conisements stud village List Mod energy potrse peritiz story hor road Pre Los Rep\n"
     ]
    }
   ],
   "source": [
    "# Generate a new sample\n",
    "@torch.no_grad()\n",
    "def generate_sample(input):\n",
    "    t1 = torch.tensor(encode(input), dtype=torch.long, device=device) # Tokenize string -> (tensor of ids)\n",
    "    t1 = t1[None,:]  # (1 , [size of ids])\n",
    "    newgen = model.generate(t1,max=64)[0].tolist() # call the generate method, limit output size\n",
    "    result=decode(newgen) # decode the result with the tokenizer to get back characters\n",
    "    print(f\"{result}\")\n",
    "\n",
    "generate_sample(\"The mountain in my city is\") # Generate a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7z9sOljjvq2l"
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Main Training Process\n",
    "#################################################################################\n",
    "\n",
    "# Set Weight Decay differently for different kinds of parameters\n",
    "# parameter dictionary where keys are parameter names, and values are the parameter themselves\n",
    "p_dict = {p_name: p for p_name, p in model.named_parameters() if p.requires_grad} # len: 370\n",
    "\n",
    "# isolate weight matrices as they benefit specially from weight decay\n",
    "weight_decay_p = [p for n, p in p_dict.items() if p.dim() >= 2]  # len: 171\n",
    "\n",
    "# isolate other parameters like bias parameters, that don't benefit from weight decay\n",
    "no_weight_decay_p = [p for n, p in p_dict.items() if p.dim() < 2] # len: 199\n",
    "\n",
    "# store the parameter types in a list of dictionaries\n",
    "optimizer_groups = [\n",
    "    {'params': weight_decay_p, 'weight_decay': weight_decay},\n",
    "    {'params': no_weight_decay_p, 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# Declare optimizer, it helps us compute gradients, update parameters, manage learning rate, apply weight decay\n",
    "optimizer = torch.optim.AdamW(optimizer_groups, lr=lr, betas=(0.9, 0.99))\n",
    "# betas: control the exponential moving averages of the gradient and its square,\n",
    "# which are essential components of the Adam and AdamW optimization algorithms.\n",
    "\n",
    "# Declare scheduler to change learning rate through the training\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_iters, eta_min=lr/10)\n",
    "# learning rate will descend till a minimum of a tenth of the lr\n",
    "\n",
    "start_iteration = 0\n",
    "best_val_loss = float('inf')  # Track best loss value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "YXt7xGMvwQ_5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM - Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x1/5s7srkw11qn8zndqtnz1w43r0000gn/T/ipykernel_3120/2082066746.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded iter 9600 with loss 2.84375\n"
     ]
    }
   ],
   "source": [
    "# Loading Checkpoints\n",
    "\n",
    "# Loads a previously saved checkpoint\n",
    "def load_checkpoint(path):\n",
    "    print(\"LLM - Loading model\")\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict']) # Load parameters\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict']) # Load optimizer state\n",
    "    iteration = checkpoint['iteration'] # In what iteration did we save the model?\n",
    "    loss = checkpoint['loss'] # What was the last loss value?\n",
    "    print(f\"Loaded iter {iteration} with loss {loss}\")\n",
    "    return iteration, loss\n",
    "\n",
    "################# OPTIONAL : LOAD A PREVIOUS CHECKPOINT\n",
    "if os.path.exists(f\"{checkpoint_dir}/{checkpoint_load_fn}\") and load_pretrained:\n",
    "    start_iteration, loss = load_checkpoint(checkpoint_dir + checkpoint_load_fn)\n",
    "    best_val_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "B2KPyh1cwo3t"
   },
   "outputs": [],
   "source": [
    "#### INFERENCE MODE - Activate inference and then exit\n",
    "if inference==True:\n",
    "    model.eval()\n",
    "    while True:\n",
    "         qs = input(\"Enter text (q to quit) >>> \")\n",
    "         if qs == \"\":\n",
    "             continue\n",
    "         if qs == 'q':\n",
    "             break\n",
    "         generate_sample(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "2zUtal8zwsUl",
    "outputId": "435ac1e0-8bb5-45f6-d17a-8f81e4e444d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9600: train loss: 3.1197917461395264 / val loss: 2.9114582538604736\n",
      "The mountain in my city is an urban area in 80. It was named during the row of .\n",
      "\n",
      "Neo, in the end of the, cantons, Saint John (the state of Alpine – Arpine in 1929) was the first people to start the name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 50/5400 [01:26<2:01:02,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9650: train loss: 2.96875 / val loss: 3.0833332538604736\n",
      "The mountain in my city is a three-through the world dog's Joe Philippines.*Other professor Robert Perth hospital were opened to about 45 days in Muso, the Joe Bell. Then Flames should be included at Joe La roford Palace, 880 intended to the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 100/5400 [02:47<1:58:04,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9700: train loss: 2.984375 / val loss: 3.125\n",
      "The mountain in my city is from the east Sea of Vegar River Island.\n",
      "\n",
      "The port is \"Mentitarne\", an astric–beautifular Para of Plan Winstam, and about 3,000 town of Deuter Sant Mint-Rhuce Tre\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 150/5400 [04:06<1:49:44,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9750: train loss: 3.1302082538604736 / val loss: 3.0572917461395264\n",
      "The mountain in my city is Coolier. In 2061,814 people lived there. Arkansas was called it the \"Kescentennonds\". The Branch of the Arodcastle began in 1777. In 1866, in the 19th century,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 200/5400 [05:26<1:49:25,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9800: train loss: 3.0625 / val loss: 3.046875\n",
      "The mountain in my city is about 17 people. Morn Institute in Tamil Statistic is a private name. Hodg was founded in January 1889.\n",
      "\n",
      "John Risisily had the total population of 23,154 as of an example, as well, as\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 250/5400 [06:44<1:54:51,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9850: train loss: 3.0520832538604736 / val loss: 3.046875\n",
      "The mountain in my city is profession of this color. It decides more judge to the rest of the maxides. But in the past being painted using an internal stumber of the city of Moon.\n",
      "\n",
      "For severe times, it artists are often related to U.S. Instead that\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 300/5400 [07:54<1:48:36,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9900: train loss: 2.8177082538604736 / val loss: 2.96875\n",
      "The mountain in my city is 315 roadsqota covers Hertden, Bridence Production, Donald H. Paris, Ed.. The eastern half of the biggest land in ASE under 30,0000 miles into the town against the aristria. It includes the original channel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 350/5400 [09:03<1:48:13,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9950: train loss: 3.2083332538604736 / val loss: 2.9947917461395264\n",
      "The mountain in my city is about 52°. Its level ussol is large.0 million sab.\n",
      "\n",
      "Thians other only several people may have taken the largest city. Mnoday, it can be made from the city to the two categist Genera (50 m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 400/5400 [10:07<1:10:10,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10000: train loss: 3.15625 / val loss: 2.9791667461395264\n",
      "The mountain in my city is 2 and a major mountain in the Americas in the southeast of the Piedmont city. It is divided by 3,000 rivers (Emstruction is the names\"), quarter where high. . The, the region focuses on the mountains, and for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 450/5400 [10:52<1:07:37,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10050: train loss: 3.0 / val loss: 3.0885417461395264\n",
      "The mountain in my city is about north of Thun joinra's times along Sagunco Palict Hitrows.\n",
      "\n",
      "Mindri, born on September 13, 1932, Luciers at Danwan), Victoria, is a goreographic specialist for Indig works of Sanny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 500/5400 [11:37<1:06:53,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10100: train loss: 2.8854167461395264 / val loss: 3.0677082538604736\n",
      "The mountain in my city is $1,266 at that below orb Valentet J Islands. The date was 499 locked X-00 plane, which allows Kusov who had Healthy to make a lot of people.\n",
      "For two years, one of X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 550/5400 [12:22<1:06:21,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10150: train loss: 2.8489582538604736 / val loss: 3.0885417461395264\n",
      "The mountain in my city is strict, and is normal in volume being within the average.\n",
      "\n",
      "The Gulf of Seattle were to dispersal relatives of the Confederacturaas. The Family Most-finals however was sick in a racial land. It could entered the Atlantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 600/5400 [13:07<1:05:30,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10200: train loss: 3.2083332538604736 / val loss: 3.171875\n",
      "The mountain in my city is Bakergency. It is many miles east of Bonidenc. Gasey Dylame is the leader of very large parts in cities with Rita.\n",
      "\n",
      "Average (*_87) Sheppt\n",
      "\n",
      "A Plain ultimental () is a Borough-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 650/5400 [13:51<1:04:38,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10250: train loss: 2.90625 / val loss: 3.1510417461395264\n",
      "The mountain in my city is relth (the largest population (~ per h), rises, lower community and green frequency, neutral. He is a the oldest socialist buildings in Edwardth, in a walls of historical area to the Northern area and mainland and bermark. \n",
      "\n",
      "He was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 700/5400 [14:36<1:04:35,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10300: train loss: 3.1875 / val loss: 2.9791667461395264\n",
      "The mountain in my city is in the lake in the northeastern part of southwestern the Leburgh, northwestern part of the lake of Gentina Ferry, the largest city of Grande Ĉates.\n",
      "\n",
      "Inwards there are three castles:\n",
      "\n",
      "Most important schools that use their home high\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 750/5400 [15:21<1:03:31,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10350: train loss: 3.0052082538604736 / val loss: 3.1510417461395264\n",
      "The mountain in my city is 2.5 kilometers in the parorough. The landscape is defined from the lake in the city. It is small by an average of 35 meters (9.3 m), tall. The moiz–of lecta was the largest land plants\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 800/5400 [16:06<1:02:38,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10400: train loss: 3.1145832538604736 / val loss: 3.234375\n",
      "The mountain in my city is of the ancient Romanaces. The river is thought to be an official name in a church in Rome-et-de-Nothë.\n",
      "\n",
      "In the historical G the Akrë lives in Tburgstl. Now the Gillard has these choices arena's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 850/5400 [16:51<1:02:14,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10450: train loss: 2.9427082538604736 / val loss: 2.9010417461395264\n",
      "The mountain in my city is about 18 BC,000 years ago, 21 km (85 left) land sacramby. The lake is .\n",
      "\n",
      "One university and�wls are married to family, in Eastern England and are animated kids. Most of this including Voivury\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 900/5400 [17:35<1:01:21,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10500: train loss: 3.0052082538604736 / val loss: 3.0104167461395264\n",
      "The mountain in my city is along the minilet world Mountains, the Steven Romans and Cleveland in Biebothrärtu in Vermany-Halesbothrörte.\n",
      "\n",
      "\n",
      "Afina\n",
      "\n",
      "Afina is a 2020 Ukr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 950/5400 [18:19<58:27,  1.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10550: train loss: 2.9947917461395264 / val loss: 2.984375\n",
      "The mountain in my city is 36 miles and 250 kilometres west of the city.\n",
      "\n",
      "The Pope Alps is named after the great Division of Vladiman and the city of Nasharian railway station in Phrumbi Region, about three Gulf of Nashical Highway era\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 1000/5400 [19:02<57:49,  1.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10600: train loss: 3.046875 / val loss: 3.09375\n",
      "The mountain in my city is Amaz Western III. The capital Aifa Island is the seat of the Parish and the Isa consistoric US and populated area of the city of Tight, at the Togo River Mountake.\n",
      "\n",
      "The Chican Mountains borders the border from the Guadal Sun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1050/5400 [19:46<57:09,  1.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10650: train loss: 3.09375 / val loss: 3.0\n",
      "The mountain in my city is .\n",
      "\n",
      "The old province is named after the Eta, backing as the arder into Cha, antipheri 1. N. kilospitals are of Arabs. The altar of analogific Oches (Presychi, Austria), traditions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1100/5400 [20:30<58:03,  1.23it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10700: train loss: 3.1354167461395264 / val loss: 3.1770832538604736\n",
      "The mountain in my city is thus the rest of the Junior Valley, Montana.\n",
      "Some of the towns of the bus schools in Rosario are Polertdia, and Tehleches.\n",
      "\n",
      "Hocalonia Herswit the lake others have shown G are one of the major villages of I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 1150/5400 [21:15<56:41,  1.25it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10750: train loss: 3.1145832538604736 / val loss: 3.0\n",
      "The mountain in my city is Tatt of the Nicoscium Formula\" (after Iraq). The thief town is buried in the forests, as it is Levin. \n",
      "\n",
      "The main land in Oakland moistakes, nories and gluces in the.\" App\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1200/5400 [22:00<58:29,  1.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10800: train loss: 3.0833332538604736 / val loss: 2.8072917461395264\n",
      "The mountain in my city is the largest city in the world.\n",
      "\n",
      "It is the local supporting railway in the country. It is a small part of the Grand Laameda River. The Traditionard is in the lower area of two districts: the largest town.\n",
      "\n",
      "Life, North Rhine County, United States\n",
      "[CHECKPOINT]: Saving with loss:  2.8072917461395264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1250/5400 [37:54<57:28,  1.20it/s]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10850: train loss: 3.15625 / val loss: 2.96875\n",
      "The mountain in my city is a giant spidal unacts plynthesis period. There is one shaped like AlHoffy's 45th past was a rocket on a tree eaten glass about 552 mm. A rum recron field for other centers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1300/5400 [54:55<56:55,  1.20it/s]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10900: train loss: 3.0260417461395264 / val loss: 2.984375\n",
      "The mountain in my city is called Aaga.\n",
      "\n",
      "\n",
      "Jas-de-Loum\n",
      "\n",
      "Jas-res-de- Lagtern (born 18 January 1911) is an Renaih�ue Rica. He is best known for which he was one of the whole of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1350/5400 [55:40<56:19,  1.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10950: train loss: 3.1510417461395264 / val loss: 3.0416667461395264\n",
      "The mountain in my city is in the island. Saiv tributaries are still still in a sister equatic pictures, inside the islands of the Virgin Rural Warri. Throughout the imported a way of the land - and accepts there are possibly other monanolas, heading the p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 1400/5400 [56:26<55:49,  1.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11000: train loss: 2.9375 / val loss: 3.0104167461395264\n",
      "The mountain in my city is the capital of the cities of Whens Ambil the rivers. It is one of three subt raner and one of three major countries to have a long land named kingdom. The city has an a normal religion with 14 region ofotanes. The country in these rivers are:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 1450/5400 [57:12<55:07,  1.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11050: train loss: 3.1666667461395264 / val loss: 3.0104167461395264\n",
      "The mountain in my city isn't the Chandy in the world. The main mountains are the quarters of Tajuk. The spots are in the highest spots underground bus sea level. \n",
      "\n",
      "The motorfter temporarily crater Prat., at the Turx to give them\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1500/5400 [57:58<54:28,  1.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11100: train loss: 2.8541667461395264 / val loss: 2.9739582538604736\n",
      "The mountain in my city is located in the state of East Philadelphia. From 1950 to 1951, Beach defeated the city's English government invads. He made the organisation of Barbana Degero in the upby weeks of 1975. By\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 1550/5400 [58:43<53:30,  1.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11150: train loss: 3.0052082538604736 / val loss: 2.9375\n",
      "The mountain in my city is or focuses on the story. The pit down or very highways in the city. The trating center is organised in Technical Anti-Ankhout, in a rare formized value where Ledigac cs plants were very t cancer. An evidence of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 1600/5400 [59:29<53:00,  1.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11200: train loss: 2.8854167461395264 / val loss: 3.0364582538604736\n",
      "The mountain in my city is 1.1 hurricane goes hunt the town by the architect Olympique Paisetbach. The Borique, formed in 2001, visited in the town of Sina Lahima. It crosses a Brazilian city with the name Inuo Pto�\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 1650/5400 [1:00:15<51:57,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11250: train loss: 2.984375 / val loss: 2.96875\n",
      "The mountain in my city is Hadeleka. Today there are 3539 people living under the Ra of Baden. It is a Christian Melessel. Admir is usually seen in the lake of latercome, called \"steralms\" (womenburst) is calledst) is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 1700/5400 [1:01:01<51:44,  1.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11300: train loss: 2.921875 / val loss: 3.0052082538604736\n",
      "The mountain in my city is in the Southern Ocean.\n",
      "\n",
      "Historians can rise all of patients are:\n",
      "\n",
      "\n",
      "Jsenda X\n",
      "\n",
      "Jsenda This is at Prevalium on the American secret.\n",
      "It is one of the papers being on average and below\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 1750/5400 [1:01:47<50:58,  1.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11350: train loss: 3.015625 / val loss: 3.1614582538604736\n",
      "The mountain in my city is about a metals. The Atom Kritiouse is no holted in Anatolia. Lucia is about 2 sð , Columbus's territory, and the third largest of the city in Italy.\n",
      "\n",
      "Association are scholars such as the nickn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1800/5400 [1:02:33<50:15,  1.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11400: train loss: 3.0260417461395264 / val loss: 3.0833332538604736\n",
      "The mountain in my city is small that wides a large portion to the south of the region: the Papân do a means that pick walking or in the country Divis Ulyss fill the nortland region under the close Bill Taylan Revolution.\n",
      "\n",
      "The top of the Parese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 1850/5400 [1:03:18<49:20,  1.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11450: train loss: 2.9791667461395264 / val loss: 2.9479167461395264\n",
      "The mountain in my city is Romania.\n",
      "\n",
      "The Guardicia was named after a placcy at Osbre Radzlorium, due to the Angeles coastal section.\n",
      "\n",
      "The Cuban word “V'” includes air-tatchhead coordinated -1 million events in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 1900/5400 [1:04:04<49:33,  1.18it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11500: train loss: 2.9791667461395264 / val loss: 2.9947917461395264\n",
      "The mountain in my city is Sucino. It is one of the four communes of the principal City castle peoples in Copotala is located in Mungano. It is named after Maupangariro of the Mukwa and his Zhivgeon, the conquickly charges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 1950/5400 [1:04:59<1:13:22,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11550: train loss: 2.9114582538604736 / val loss: 3.0260417461395264\n",
      "The mountain in my city is the capital of Yu Ahuckhattaay, Tarajika on the easternmost part of Kambakhazar. As Kammakhoka, Kammungya, South Munawur, Flora, Kilmya, also Yuju, Don\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 2000/5400 [1:08:13<3:30:25,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11600: train loss: 2.8125 / val loss: 3.046875\n",
      "The mountain in my city is the world's largest enough. \n",
      "\n",
      "Abition leased here for the Gram more than 2060 centuries.\n",
      "\n",
      "Some on the precipitation of the prophetoxes had a dark cycle and spines (boured cury) were discovered in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 2050/5400 [1:11:04<2:51:47,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11650: train loss: 2.9114582538604736 / val loss: 2.9375\n",
      "The mountain in my city is the Creek of the Sun.\n",
      "\n",
      "Peoulrait (, response)\n",
      "\n",
      "In Guine embander, also known as Mundär\" or \"Arctos läي\" (), is a origins of Vatican target, a study\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 2100/5400 [1:13:14<1:35:41,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11700: train loss: 3.0885417461395264 / val loss: 2.8802082538604736\n",
      "The mountain in my city is about 11 different,00,000 people. The mountains are in the total Dallosiou.\n",
      "\n",
      "In the first period, the city was sends to this place to mannerlets for a tissue. In night, the Japanese remaining Chinese owner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 2150/5400 [1:14:35<1:14:49,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11750: train loss: 2.9635417461395264 / val loss: 3.0416667461395264\n",
      "The mountain in my city is known for its guy overwally offscouring and funded by the Armachian calendar.\n",
      "\n",
      "\n",
      "\n",
      "Villegu\n",
      "\n",
      "Villegu (June 21, 1939 – March 14, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 2200/5400 [1:15:51<1:14:48,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11800: train loss: 2.8333332538604736 / val loss: 2.96875\n",
      "The mountain in my city is when it is called the \"Marburee\".\n",
      "\n",
      "Inلٷَ (\"Châsrex vi Vi\") a longer mountain in Alexandria, Barge is the song by the Red Mystery. He is the second second of the band. They\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 2250/5400 [1:17:17<1:37:22,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11850: train loss: 3.015625 / val loss: 3.0052082538604736\n",
      "The mountain in my city is 'Gliskeeper'.\n",
      "\n",
      "Alexander, Switzerland\n",
      "\n",
      "Alexander is the third Greek village in the United Kingdom. Black Valley, and a certain continues a spiritical family.\n",
      "\n",
      "Batie, Erne-S Wustler\n",
      "\n",
      "Batie has\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 2300/5400 [1:19:29<2:25:59,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11900: train loss: 2.8958332538604736 / val loss: 2.9375\n",
      "The mountain in my city is \"Acea\" (\"a Dakhatwoman\") or \"Agni\" (provence).\n",
      "\n",
      "\n",
      "\n",
      "Meremon Eventa is the most influence of historical bishop Georgian music who are said to be a pleum forces that suggested that she\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 2350/5400 [1:21:23<1:20:32,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11950: train loss: 3.0208332538604736 / val loss: 3.0\n",
      "The mountain in my city is saliest shrub ( km) and side of the river of the Filipura State.\n",
      "\n",
      "BN Championship counts of the city of Florenzbek is in the centre of the River Apple. \n",
      "\n",
      "\n",
      "\n",
      "H Kras, formerly known for,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 2400/5400 [1:22:41<1:06:18,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12000: train loss: 2.8541667461395264 / val loss: 2.8802082538604736\n",
      "The mountain in my city is located on . The an error track attempt to the Maple Le seen the mountain for an notations the Soviet victory charge on 11 January 1948. The term is heaviest, as a stronger started after Moscow, saw many people.\n",
      "\n",
      "E\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 2450/5400 [1:23:55<1:07:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12050: train loss: 2.8020832538604736 / val loss: 3.109375\n",
      "The mountain in my city is grown and has a palicence some dremior. Queen Elizabeth Queen Elizabeth II, landfareing or said to dry and Queen woman. Queen Elizabeth during the TV in the Middle Ages calendaristic attacks using the National Borus Frank who\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 2500/5400 [1:25:18<1:33:38,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12100: train loss: 2.984375 / val loss: 2.8697917461395264\n",
      "The mountain in my city is a subplicability into the middle of Lake Manhattan.\n",
      "\n",
      "Bahare Fernandilly\n",
      "\n",
      "Bahare Gene Fernandella Fernandette (born October 7, 1998) is a Canadian actor, author, creator and singer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 2550/5400 [1:27:13<1:53:15,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12150: train loss: 2.9375 / val loss: 3.2604167461395264\n",
      "The mountain in my city is the largest tropical length on the island, and west-hammar of the Patrose Age.\n",
      "\n",
      "Namchell discovered the island on Canter, an Italian river squad that consisted of the northern depression has been an one-sarrier like an historical trade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 2600/5400 [1:29:32<1:58:56,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12200: train loss: 2.9635417461395264 / val loss: 3.1041667461395264\n",
      "The mountain in my city is linked, balon mijun peace.\n",
      "\n",
      "Mrin Sierblue\n",
      "\n",
      "Mrin Sierstra Loreyclere responses Italyity muscle equation.g. a year in the Romanief deities.\n",
      "\n",
      "Mrin Sierbr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 2650/5400 [1:31:57<2:01:05,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12250: train loss: 3.1197917461395264 / val loss: 3.15625\n",
      "The mountain in my city is famous for CanM Gunfhall, a Romanian family.\n",
      "\n",
      "\n",
      "\n",
      "Gømabadka\n",
      "\n",
      "The G�˝mabad Isil (; Romanisic: гиа麄ла йи�\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2700/5400 [1:34:09<1:48:39,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12300: train loss: 2.9270832538604736 / val loss: 2.8645832538604736\n",
      "The mountain in my city is esttime. The older universityens away district has an area of the altage and cultiven mine sea mine and silica.\n",
      "\n",
      "\n",
      "Kriffalzia Isjade is a group of islands with Suria's astatic coats. The government is the nation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 2750/5400 [1:36:14<1:37:29,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12350: train loss: 2.9114582538604736 / val loss: 3.0572917461395264\n",
      "The mountain in my city is about Newton Genton in Barris bought from Newton, in 1811. Ossman is there of continued to connect two longience groups of emperor Mozéoutner Gooda. \n",
      "\n",
      "Hubourne is maintaining our\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 2800/5400 [1:38:31<1:53:40,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12400: train loss: 2.9791667461395264 / val loss: 3.0572917461395264\n",
      "The mountain in my city is known for that Christonic, so Cox sizes the neighbouring Funich, the \"Phlischie mirth86 and Jane Han\" painterifest ach of ritenging spring. In this rit\n",
      "\n",
      "Before 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 2850/5400 [1:39:53<54:47,  1.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12450: train loss: 2.9270832538604736 / val loss: 2.9791667461395264\n",
      "The mountain in my city is how to fall over water. As the royers are cold, in the water is an oce tree. The river gets in the sea m survivor.\n",
      "\n",
      "The brack is more people (like floot). The prodness of the arctic are simply fred back to�\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 2900/5400 [1:41:04<53:31,  1.28s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12500: train loss: 2.8645832538604736 / val loss: 2.9427082538604736\n",
      "The mountain in my city is known a dosper of the Mohamanas, a sung one-fu antimenses named after Greek mythologian Richard II.\n",
      "\n",
      "\n",
      "\n",
      "Gerazattic Amgen\n",
      "\n",
      "Gerodol Mohamal Lamaes (20 September 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 2950/5400 [1:42:20<52:52,  1.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12550: train loss: 3.0885417461395264 / val loss: 3.0052082538604736\n",
      "The mountain in my city is near the river. On all of the river came from the to mainland north of the fewest near and; civil damage is very difficult to ice. To the south were Afghans and also surround the lowst forest site are furgitor of the river.\n",
      "\n",
      "Skautz are one of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 3000/5400 [1:43:38<56:11,  1.40s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12600: train loss: 3.0104167461395264 / val loss: 3.0625\n",
      "The mountain in my city is one of the largest cities of the Ippreaches and suburbs is located in the other United Kingdom. It is one of the few \"Normula of Florench elbemutski\". It is about Korean Theatre.\n",
      "\n",
      "It is located aboutound after Noulin/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 3050/5400 [1:44:58<53:18,  1.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12650: train loss: 3.03125 / val loss: 2.84375\n",
      "The mountain in my city is named after Edaha. No complete-states people in carry majority, religion, and exhibit. Most of the level is difficult to change, but this companies look like economic activities.\n",
      "\n",
      "In the 2001 census, 45 inhabitants and four centuries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 3100/5400 [1:46:18<52:00,  1.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12700: train loss: 3.1458332538604736 / val loss: 2.9114582538604736\n",
      "The mountain in my city is in Engula Isarma, island of the province of Yanananananat de Bozki. This atom in the time is up to 13 separate river kmªtlanti (Goritmar). The River Weye borders Yanana to D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 3150/5400 [1:47:39<50:39,  1.35s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12750: train loss: 2.9739582538604736 / val loss: 3.078125\n",
      "The mountain in my city is 506% of allmater sections.\n",
      "\n",
      "The word \"falome\" means \"falomegial\" meaning \"\"falry\"G\" is in \"vertol\". The book comes from Nobel mathematics and Latin (tradicist) that c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 3200/5400 [1:49:00<50:36,  1.38s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12800: train loss: 3.0052082538604736 / val loss: 2.8333332538604736\n",
      "The mountain in my city is Tulisburg. Its altitude is Louis County Road.\n",
      "\n",
      "Tulis Tulis\n",
      "\n",
      "The Tulis Tulis (PATS) is an American rock band from Astron San Gabstiv7. It was established in 1784.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3250/5400 [1:50:21<49:23,  1.38s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12850: train loss: 2.953125 / val loss: 2.890625\n",
      "The mountain in my city is about the Arab State's mountain; the official basins, in the borders with the region. Polyone City was useful today until it became one of the cities at the time (Pastouse 10) in Italy in April 1924.\n",
      "\n",
      "The area is var\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 3300/5400 [1:51:42<47:40,  1.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12900: train loss: 2.9114582538604736 / val loss: 3.03125\n",
      "The mountain in my city is for industrial irrel n mayor from the base (unfect day, writing), to financely until the Roman ch'odeceptalitude is about 94 merg. \n",
      "\n",
      "Hisan close size is weighted from the past selection Peace Mies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 3350/5400 [1:53:04<46:34,  1.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12950: train loss: 2.875 / val loss: 3.1197917461395264\n",
      "The mountain in my city is of about 24,343 metres. Although 8,8200 people lived there and there were 85,643 people living there.\n",
      "\n",
      "National Parker\n",
      "\n",
      " Department Page, announced on 5 March,995, it was added to the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 3400/5400 [1:54:25<45:52,  1.38s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13000: train loss: 2.9635417461395264 / val loss: 3.015625\n",
      "The mountain in my city is 220. The names of the administrative groups were: Chang and Aarlythen, 8500 residents, 155, and 76,411, 11779, but came from Olympiaet-hand, in 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 3450/5400 [1:55:46<44:14,  1.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13050: train loss: 3.0677082538604736 / val loss: 3.0364582538604736\n",
      "The mountain in my city is serited with this oxygen to speed of according to the crush of the tribut in the capital. The garuse from Uplaceuga to the planets.\n",
      "\n",
      "The Magar Autonomous Revice\n",
      "\n",
      "The Marquest for the Protection Protection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 3500/5400 [1:57:07<43:10,  1.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13100: train loss: 3.0416667461395264 / val loss: 2.7708332538604736\n",
      "The mountain in my city is Haw Lake City. The longest body on Earth was February of 2018 after Emlishops of Urganna, Vicarck Provincz, he competed in the morning of pen in winter. In 1946 the creation of the USA received K\n",
      "[CHECKPOINT]: Saving with loss:  2.7708332538604736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 3550/5400 [1:58:30<41:42,  1.35s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13150: train loss: 3.0104167461395264 / val loss: 2.8854167461395264\n",
      "The mountain in my city is Anto in the northwest.\n",
      "\n",
      "Its area for days is incorrectioned by the Spiremman Military Resouth area.\n",
      "\n",
      "\n",
      "They had the main temperatures when men lived at big front (of have seats) except in the parrows, turba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 3600/5400 [1:59:51<40:51,  1.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13200: train loss: 2.7760417461395264 / val loss: 3.0052082538604736\n",
      "The mountain in my city is a mountain range in the Province of Manawayal. It flows throughout the world from one to the area close to German.\n",
      "\n",
      "Sign comes from Antrõe, about only 100�ences bring foods. Tehsop's breedail when they be\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 3650/5400 [2:01:13<39:46,  1.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13250: train loss: 2.9427082538604736 / val loss: 2.984375\n",
      "The mountain in my city is Nichi Wrihan, (uly, , , , 14,25, 157,91)day illeem Kansas. The census is part of two councils: Lane Hection, and Smahn-Bakiafe Club and L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 3700/5400 [2:02:34<38:48,  1.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13300: train loss: 3.0833332538604736 / val loss: 2.9895832538604736\n",
      "The mountain in my city is found at Queen have their great patent people about heritage in the region, and coditzing out to \"Peotheus\".\n",
      "\n",
      "Zain in the Alleta is about 9,400 American songs during first Irm.The name \"Zare,\" almost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 3750/5400 [2:03:55<36:50,  1.34s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13350: train loss: 2.9375 / val loss: 3.0989582538604736\n",
      "The mountain in my city is called Münois, Wilhester. She belongs to Liülf, which is the strongest city in France. A river pihhat is still spendrositan or small sculptures of Pimpans. The flows with John, the town of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3800/5400 [2:05:17<36:34,  1.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13400: train loss: 2.7604167461395264 / val loss: 2.7604167461395264\n",
      "The mountain in my city is often used to be placed on the coes of promelops.\n",
      "\n",
      "\"Sc English Kingovern\n",
      "\n",
      "\"Scouncil of Northern Territory\" is one of the thirending story starting: \"The catheden of the Brongo\" and the sailors of Cerained\n",
      "[CHECKPOINT]: Saving with loss:  2.7604167461395264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 3850/5400 [2:06:39<34:18,  1.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13450: train loss: 2.8541667461395264 / val loss: 2.9583332538604736\n",
      "The mountain in my city is in the southern part of France. Tikian tends to winone from his hivplace, where he is armoutally good. Flake runs across the surface. It then install angle in the basso. He was inside the morning of the acronym plane that\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 3900/5400 [2:08:00<33:46,  1.35s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13500: train loss: 3.0416667461395264 / val loss: 2.7760417461395264\n",
      "The mountain in my city is at Lake Camp in the Eval\" Hotel. It is aboutuce by the artist, who is also monopanoic. The second photographic opera is followed by the song of the same name.\n",
      "\n",
      "Moncónica became a tourist settlement.\n",
      "\n",
      "Mon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 3950/5400 [2:09:21<33:15,  1.38s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13550: train loss: 2.984375 / val loss: 3.0052082538604736\n",
      "The mountain in my city is very liest.\n",
      "\n",
      "The most work in the stclaimed star for a small grireme color. It could reconvertise until privately behind casting. The cathedral is now known as a th II'-guitar dorf.\n",
      "\n",
      "A male uses have been\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 4000/5400 [2:10:42<31:56,  1.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13600: train loss: 3.2083332538604736 / val loss: 2.84375\n",
      "The mountain in my city is the largest land in the state of New Orleans there. The main port area is ten miles southeast of New Brunan who is east of Kingveloper Hill. All the section of this to the south is split with the Elmius Highway: then the places the Resaciers of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 4050/5400 [2:12:03<30:25,  1.35s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13650: train loss: 2.78125 / val loss: 3.078125\n",
      "The mountain in my city is Runal Ma Abduliy is south of the lake with about 2.3 million work in Earth Presbyterian, the site port and roadly caused the building. It has a river would spend its source as part of a development. It is called Stry Bouye and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 4100/5400 [2:13:24<29:39,  1.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13700: train loss: 2.9947917461395264 / val loss: 2.9322917461395264\n",
      "The mountain in my city is about 23 million miles long, costiated by artificial serious machine airports and adogment, such as White Bush, Lood Time, Daris, and Queen conservationist the \"Jillias Limune...\", MacThe History\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 4150/5400 [2:14:46<28:24,  1.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13750: train loss: 2.796875 / val loss: 2.953125\n",
      "The mountain in my city is is one of the 512,539 paintings an arrau in Long Kong, deadmore, the museum was handled by 100 food emaths in Beirang-side had created for designation, Persane, and Zawar Al featured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 4200/5400 [2:16:07<27:12,  1.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13800: train loss: 2.7708332538604736 / val loss: 3.0260417461395264\n",
      "The mountain in my city is a dial tree that includes many funedace connection via Hykeleton and grossing blackbul gave the via ice and palane. Most at the time that the mountain is shape. To erosions of the range, we train is put down by viewers of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 4250/5400 [2:17:27<26:03,  1.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13850: train loss: 2.8489582538604736 / val loss: 2.875\n",
      "The mountain in my city is mainly above the lake of the region Mario al Platea and Great Marioh, Lionardo. It is a supermanent capital of the region in Guautyla in southwestern Safe consist of the official capital city; and a rich part of the Catalolic Gen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 4300/5400 [2:18:49<25:22,  1.38s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13900: train loss: 2.7708332538604736 / val loss: 2.921875\n",
      "The mountain in my city is size. The city is at the highest point of the River Irawal, and at 74². The capital is Lingaustan. The region is Melbourgne Stone, where it is in the Smainhouse River.\n",
      "\n",
      "As of the 201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 4350/5400 [2:20:10<23:55,  1.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13950: train loss: 2.8958332538604736 / val loss: 2.8489582538604736\n",
      "The mountain in my city is about 12 months (shield) opened in the trail. It is surrounded by a shrub, or a swrick to get the long and wet swall. It has long and inclut direct jach to the hotel. It lives like a seal to the shark.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 4400/5400 [2:21:31<22:23,  1.34s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14000: train loss: 2.953125 / val loss: 2.4791667461395264\n",
      "The mountain in my city is God . It is a right-wearing arism in love with God. The poet is the name of the character of god Manhattra Ranshenta Kau.\n",
      "\n",
      "The place has a church which comes from Wrestle is in a Tatlui Meteoricip,\n",
      "[CHECKPOINT]: Saving with loss:  2.4791667461395264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 4450/5400 [2:22:53<21:12,  1.34s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14050: train loss: 2.890625 / val loss: 2.8802082538604736\n",
      "The mountain in my city is located in the canton of Astuftsburg. It lived there in 1993. The capital is Bolberg, established in 1928. His most important part was Vi Bolberg.\n",
      "\n",
      "Headtmann nobility\n",
      "\n",
      "The collapse d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 4500/5400 [2:24:13<20:22,  1.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14100: train loss: 2.890625 / val loss: 3.0989582538604736\n",
      "The mountain in my city is when very large summitations of the easternmost island of Texas, this golfee is very preserved as it leaves in the Atlantic Ocean. Tokroving to about there are about 67,000 called \"japanite\" on the southeastern coast of central Illinois and Kansas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 4550/5400 [2:25:34<19:27,  1.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14150: train loss: 2.859375 / val loss: 3.1197917461395264\n",
      "The mountain in my city is also part of Yug Kong. In 2025, Abbey said in 2020 was Reland Batsu-Wadfaaku. It was entirely recognized by a French football team in 2020. \n",
      "\n",
      "In 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 4600/5400 [2:26:55<18:00,  1.35s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14200: train loss: 2.984375 / val loss: 2.921875\n",
      "The mountain in my city is down. The island of the level is made of a farm near the islands. The river is w 22² and one Day of the big earthy forest.\n",
      "\n",
      "Edison was made by art people, but it was the way for newspaper, politics sometimes made land proofs for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 4650/5400 [2:28:15<16:56,  1.35s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14250: train loss: 2.71875 / val loss: 3.0572917461395264\n",
      "The mountain in my city is Talaga Three\". The Tarrala won a gold medal in the 1980 Summer Olympics, Roy Wigneoff Day since 1989. Nobel Prize was published by the Director-Colais Spics Award. It is a history of a famous journal and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 4700/5400 [2:29:36<16:00,  1.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14300: train loss: 3.0260417461395264 / val loss: 2.953125\n",
      "The mountain in my city is Vengeon, it is named for a centre to the Kampaiwan Impuna general) where. \n",
      "Mindian has taken over for Dhn won and finished several years everyday years, and some have been influenced during the 18th century.\n",
      "\n",
      "M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 4750/5400 [2:30:56<13:10,  1.22s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14350: train loss: 2.9322917461395264 / val loss: 2.8645832538604736\n",
      "The mountain in my city is the historical rivers of Greece.\n",
      "\n",
      "Holesbek\n",
      "\n",
      "(Hotsbek, ) is a Council of Upper Wales. In 497, 785,843 people lived there. This is founded in 2002. In August 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 4800/5400 [2:32:12<12:21,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14400: train loss: 3.0364582538604736 / val loss: 2.9791667461395264\n",
      "The mountain in my city is where monkey took place, but also glaya or emerged bouches.\n",
      "\n",
      "Jautus’s layers has brown or shot ago and kicknames. Most of the natural and struggle is very great and painful. Abustment only create\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 4850/5400 [2:33:12<07:41,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14450: train loss: 2.9166667461395264 / val loss: 3.0625\n",
      "The mountain in my city is the top coal casted basters of the Pacific Ocean and Araghyllar Highway. The city of Barcelona has graduated from Queensland and () and then the younger brother of Calquan, Kannal, Thudril, and Alberto-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 4900/5400 [2:33:58<06:57,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14500: train loss: 3.1041667461395264 / val loss: 3.0364582538604736\n",
      "The mountain in my city is Desanal.\n",
      "\n",
      "Avesstage High School has its own degree and Armelongha\n",
      "\n",
      "Ki Moseshi University is a State University Agency theory that makes parts of Raja.\n",
      "\n",
      "Yasarapevi language\n",
      "\n",
      "Yasarapevi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 4950/5400 [2:34:43<06:15,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14550: train loss: 2.6927082538604736 / val loss: 2.765625\n",
      "The mountain in my city is 33 miles (17,201 people). The Panaji temple is the highest streets, in the Haatajata province, between 42,490 people ( Afghanistan) and 588 women (30.5%).\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 5000/5400 [2:35:29<05:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14600: train loss: 2.8802082538604736 / val loss: 3.0208332538604736\n",
      "The mountain in my city is Vastombia.\n",
      "\n",
      "It is where Pyces are the Mestone Armenian Memorial in the Russian Kingdom to celebrate the Russian Empire. It is also known as the Baloque and the Sauguedar city in Italian regions.\n",
      "\n",
      "During the Netherlands in K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 5050/5400 [2:36:15<04:51,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14650: train loss: 3.0364582538604736 / val loss: 2.7864582538604736\n",
      "The mountain in my city is when the demonunction is the total directly outside of Slov's Metal Burine Museum.\n",
      "\n",
      "Originally, on the north coast, and created the small, is in the center of Spokranamhip, named after a Catripen tube another protocade but\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 5100/5400 [2:37:01<04:11,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14700: train loss: 2.890625 / val loss: 2.875\n",
      "The mountain in my city is located below the shore of Kra basunastone.\n",
      "\n",
      "\n",
      "Together, the Assembly formation gribuemount\n",
      "\n",
      "Tog i, market started on September 12, 2021. The location is difficult to import in Bresemont since an\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 5150/5400 [2:37:46<03:26,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14750: train loss: 3.140625 / val loss: 2.9635417461395264\n",
      "The mountain in my city is Mark Eddon.\n",
      "\n",
      "\n",
      "\n",
      "The lists: Abastupa Rermanne, Agnáven Adil Nurand, Ca Reveri Bochala and Jane Mandraitz. She is the first Noble officer when a female manager becomes Use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 5200/5400 [2:38:31<02:44,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14800: train loss: 3.0625 / val loss: 3.0989582538604736\n",
      "The mountain in my city is officially area of where it was not an all of the 17th centuries regionalistic celebration. The choir was for a group of charities, a divineing, in confrying with \"Bhoo, Mook a Wall Street goinet\", an old government intent hip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 5250/5400 [2:39:17<02:06,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14850: train loss: 2.8020832538604736 / val loss: 2.9010417461395264\n",
      "The mountain in my city is on the mount sister rivers La Le Novnaotte in northern Australia. The buildings borders to the giant names team in one for an airlike by the divisions of the emergth and addition of household identity.\n",
      "\n",
      "Gi-Lean in the Canadiens\n",
      "\n",
      "The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 5300/5400 [2:40:02<01:23,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14900: train loss: 3.1666667461395264 / val loss: 2.9635417461395264\n",
      "The mountain in my city is a radius of exully:\n",
      "\n",
      "The main centre in the lake that has a length of maps in Earth, are both mitchers and one graph. Now all the water goes from Metaculum near Fox, Jim Calfac, Allan Antilles, and Pat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 5350/5400 [2:40:48<00:41,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14950: train loss: 2.9427082538604736 / val loss: 2.8229167461395264\n",
      "The mountain in my city is Khamris\n",
      "Apaching the island of Casendel's racecraft, is to the front pilact body of Langa, the largest Mubergi province and to the west, accounts, drales, swabwood trees, meat, occas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 5399/5400 [2:41:33<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14999: train loss: 3.0677082538604736 / val loss: 2.9739582538604736\n",
      "The mountain in my city is actively bebered to are the behaviourflies of molar goresprignouns. Therefore allowing it the costume of the lake. Since its mountains changes ray, far and oil pathillars do not be in about half of the city. Sent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5400/5400 [2:41:38<00:00,  1.80s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd73226d46f84f2481093c53028b9a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.037 MB of 0.037 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss/train</td><td>▇▆▂███▆▆▇▆▇▅▅▄▄▅▅▅▅▅▄▄▄▃▅▅▄▄▂▆▂▄▃▅▁▆▆▆▇▆</td></tr><tr><td>loss/val</td><td>▅▅▄▆▅█▃▄▄▇▄▄▇▄▅▃█▆▅▄▆▄▃▄▄▄▁▅▅▂▃▅▄▄▅▂▄▄▂▂</td></tr><tr><td>lr</td><td>███████████████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▃▃▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss/train</td><td>3.06771</td></tr><tr><td>loss/val</td><td>2.97396</td></tr><tr><td>lr</td><td>0.00012</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">basic-gpt2024_11_24_13_14_21</strong> at: <a href='https://wandb.ai/meinczinger-personal-use/llm_udemy/runs/bcwiqkqr' target=\"_blank\">https://wandb.ai/meinczinger-personal-use/llm_udemy/runs/bcwiqkqr</a><br/> View project at: <a href='https://wandb.ai/meinczinger-personal-use/llm_udemy' target=\"_blank\">https://wandb.ai/meinczinger-personal-use/llm_udemy</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241124_131428-bcwiqkqr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory released.\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "###################### TRAINING #################################\n",
    "#################################################################\n",
    "\n",
    "try:\n",
    "    for i in tqdm(range(start_iteration, train_iters)):\n",
    "        xb,yb = get_batch(\"train\") # Get a new batch of data\n",
    "        logits,loss = model(xb,yb) # Run the LLM and get the logits and the loss\n",
    "\n",
    "        if (i % eval_interval==0 or i == train_iters-1): # Calculate the loss\n",
    "            l = calculate_loss()\n",
    "            print(f\"\\n{i}: train loss: {l['train']} / val loss: {l['eval']}\")\n",
    "\n",
    "            # We do a quick test so that we observe the evolution through the training\n",
    "            # Remember that we use a very small dataset which doesn't include all topics\n",
    "            generate_sample(\"The mountain in my city is\") # Generate a sample\n",
    "\n",
    "            if l['eval'] < best_val_loss: # If we improved the best loss, save a checkpoint\n",
    "                best_val_loss = l['eval']\n",
    "                print(\"[CHECKPOINT]: Saving with loss: \", best_val_loss)\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': best_val_loss,\n",
    "                    'iteration': i,\n",
    "                }, checkpoint_dir + checkpoint_fn)\n",
    "\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                    \"loss/train\": l['train'],\n",
    "                    \"loss/val\": l['eval'],\n",
    "                    \"lr\": scheduler.get_last_lr()[0],\n",
    "                },\n",
    "                step = i)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True) # Reset gradients\n",
    "        loss.backward() # Calculate new gradients\n",
    "\n",
    "        # This line clips the gradients to prevent the exploding gradient problem during training.\n",
    "        # Exploding gradients can occur when gradients become too large, causing unstable updates to model weights.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "\n",
    "        optimizer.step() # Update the model parameters\n",
    "        scheduler.step() # Update the learning rate value\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Cleaning up...\")\n",
    "\n",
    "finally:\n",
    "    # Release GPU memory\n",
    "    torch.mps.empty_cache()\n",
    "    print(\"GPU memory released.\")\n",
    "\n",
    "if wandb_log:   \n",
    "    wandb.finish()\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "# Code designed by Javier ideami\n",
    "# ideami.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnYRMN1-xAtK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "connectx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
